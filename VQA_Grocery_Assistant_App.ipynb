{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francisco-renteria-rios/VQA_Assistance/blob/main/VQA_Grocery_Assistant_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# VQA Grocery Assistant\n",
        "\n",
        "This notebook implements an app that read a labeled image, asks a question about the image, and get the predicted answer.\n",
        "<br>The app uses a strong, pre-trained model (BLIP-2) to answer questions about grocery labels, but first extracts text using EasyOCR and includes that text in the model's prompt.\n",
        "This significantly improves accuracy when reading small, dense text like ingredient lists."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## We need to install some Dependencies\n",
        "We need `transformers` for the VQA model (BLIP-2), `accelerate` for performance, `easyocr` for the text extraction, and `datasets` to load the VizWiz data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install_code",
      "metadata": {
        "id": "install_code"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "# EasyOCR will download language models on first use.\n",
        "!pip install -q transformers datasets accelerate torch torchvision pillow tqdm easyocr\n",
        "import torch\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import easyocr\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import io\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the 'HF_TOKEN' secret and store it in an environment variable\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup and Model Loading\n",
        "We will load the pre-trained BLIP-2 model and the EasyOCR reader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_code",
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "#from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "import easyocr # Added this import\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load BLIP-2 Model and Processor\n",
        "try:\n",
        "    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "    # Load model with float16 for reduced memory usage on GPU\n",
        "    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n",
        "    model.to(device)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading BLIP-2 model. Falling back to CPU/float32. Error: {e}\")\n",
        "    # Fallback to CPU for demonstration if necessary\n",
        "    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "    model.to(device)\n",
        "\n",
        "# Load EasyOCR Reader (English, common for grocery labels)\n",
        "ocr_reader = easyocr.Reader(['en'], gpu=True if device=='cuda' else False)\n",
        "print(\"Models and OCR reader loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ocr_hybrid",
      "metadata": {
        "id": "ocr_hybrid"
      },
      "source": [
        "## OCR + Image Pipeline\n",
        "This python function implements the key logic: extract text, then build a better prompt.\n",
        "\n",
        "**Prompt Structure:**\n",
        "<br>`Question: [User Question]?\n",
        "Extracted Text: [OCR Text]\n",
        "Answer:`\n",
        "<br>This approach helps the VQA model handle dense, small text by providing the text directly as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ocr_hybrid_code",
      "metadata": {
        "id": "ocr_hybrid_code"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_ocr_text(image):\n",
        "    \"\"\"Extracts all text from the PIL Image using EasyOCR and returns a single string.\"\"\"\n",
        "    # Convert PIL Image to numpy array, as EasyOCR sometimes prefers it or has issues with direct PIL objects\n",
        "    image_np = np.array(image)\n",
        "    # EasyOCR returns a list of (bbox, text, confidence)\n",
        "    results = ocr_reader.readtext(image_np)\n",
        "    # Concatenate all text results into one string\n",
        "    extracted_text = ' '.join([text for (bbox, text, conf) in results])\n",
        "    return extracted_text\n",
        "\n",
        "def hybrid_vqa(image, question):\n",
        "    \"\"\"Performs VQA using the image and an OCR-augmented prompt.\"\"\"\n",
        "    # 1. OCR Extraction\n",
        "    ocr_output = get_ocr_text(image)\n",
        "    # 2. Hybrid Prompt Construction\n",
        "    prompt = f\"Question: {question}? Extracted Text: {ocr_output} Answer:\"\n",
        "    # 3. Model Inference\n",
        "    # Note: Using float16 for efficiency, relies on GPU being available\n",
        "    inputs = processor(image, prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "    # Generation configuration\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=50, num_beams=5, early_stopping=True)\n",
        "    # Decode the answer\n",
        "    answer = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return answer, ocr_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download a VizWiz VQA subset of 100 items from Hugging Face"
      ],
      "metadata": {
        "id": "M6BXfOprhNqi"
      },
      "id": "M6BXfOprhNqi"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Python Code to Load the First 100 VizWiz Entries (Hugging Face)\n",
        "!pip install -q datasets pandas\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Define the dataset name and split\n",
        "HF_DATASET_NAME = \"lmms-lab/VizWiz-VQA\" # Example dataset ID, actual IDs may vary\n",
        "SPLIT_NAME = \"val\"\n",
        "SUBSET_SIZE = 100\n",
        "\n",
        "print(f\"Loading first {SUBSET_SIZE} metadata entries from {HF_DATASET_NAME}...\")\n",
        "\n",
        "try:\n",
        "    # Load only the first 100 items of the 'train' split\n",
        "    # The 'datasets' library handles efficient loading of metadata/links\n",
        "    full_dataset_split = load_dataset(HF_DATASET_NAME, split=f\"{SPLIT_NAME}[:{SUBSET_SIZE}]\")\n",
        "\n",
        "    print(f\"Successfully loaded {len(full_dataset_split)} metadata entries.\")\n",
        "\n",
        "    # Convert to pandas DataFrame\n",
        "    df_subset = pd.DataFrame(full_dataset_split)\n",
        "\n",
        "    # Save the metadata to a CSV file in Colab\n",
        "    output_filename_csv = \"vizwiz_metadata_subset_100.csv\"\n",
        "    df_subset.to_csv(output_filename_csv, index=False)\n",
        "    print(f\"Metadata saved to '{output_filename_csv}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred: {e}\")\n",
        "    print(\"Please check the exact dataset ID on Hugging Face as variations exist.\")\n"
      ],
      "metadata": {
        "id": "p61hlCuuhO1x"
      },
      "id": "p61hlCuuhO1x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "data_load",
      "metadata": {
        "id": "data_load"
      },
      "source": [
        "## Load small VizWiz Dataset Samples\n",
        "To speed up testing and avoid downloading the entire VizWiz validation set, we will use the `streaming=True` feature and then take a small random sample of the first few hundred entries. This is much faster than downloading the whole split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data_load_code",
      "metadata": {
        "id": "data_load_code"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datasets import load_dataset\n",
        "NUM_SAMPLES = 10\n",
        "MAX_INITIAL_FETCH = 500\n",
        "# Fetch up to 500 initial samples to choose 10 random ones\n",
        "random.seed(42)\n",
        "# for reproducibility\n",
        "\n",
        "print(f\"Downloading a small, random sample of {NUM_SAMPLES} images from VizWiz...\")\n",
        "# 1. Load the validation split in streaming mode\n",
        "#   vizwiz_stream = load_dataset(\"HuggingFaceM4/VizWiz\", split='validation', streaming=True)\n",
        "vizwiz_stream = load_dataset(\"lmms-lab/VizWiz-Caps\", split='val', streaming=True)\n",
        "# 2. Take the first MAX_INITIAL_FETCH samples and convert to a list\n",
        "initial_samples = []\n",
        "for i, sample in enumerate(vizwiz_stream):\n",
        "    if i >= MAX_INITIAL_FETCH:\n",
        "        break\n",
        "    initial_samples.append(sample)\n",
        "# 3. Randomly select the desired number of samples from the fetched list\n",
        "if len(initial_samples) < NUM_SAMPLES:\n",
        "    # Fallback if the initial fetch was too small (unlikely for VizWiz)\n",
        "    test_samples = initial_samples\n",
        "    print(f\"Warning: Only found {len(initial_samples)} samples.\")\n",
        "else:\n",
        "    test_samples = random.sample(initial_samples, NUM_SAMPLES)\n",
        "    print(f\"Successfully loaded and sampled {len(test_samples)} examples from the streamed data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test",
      "metadata": {
        "id": "test"
      },
      "source": [
        "## Automated Testing Loop\n",
        "This loop runs the `hybrid_vqa` function on the 10 selected VizWiz samples and prints the results, showing the predicted answer against the ground truth answer (the most frequent answer provided by human annotators)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "upload_and_test",
      "metadata": {
        "id": "upload_and_test"
      },
      "outputs": [],
      "source": [
        "#!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "print(\"\\n\" + \"*\"*60)\n",
        "print(f\"STARTING VQA TEST ON {len(test_samples)} VIZWIZ SAMPLES\")\n",
        "print(\"*\"*60)\n",
        "# Inspect the keys\n",
        "print(sample.keys())\n",
        "for i, sample in tqdm(enumerate(test_samples), total=len(test_samples)):\n",
        "    # Get the image (PIL format) and question\n",
        "    image = sample['image'].convert('RGB')\n",
        "    #question = sample['question']\n",
        "    question = sample['caption']\n",
        "    # The ground truth answer is the most frequent answer from the human annotators\n",
        "    #ground_truth = sample['answers'][0]['answer']\n",
        "    # The VizWiz-Caps dataset provides 'caption' as the primary textual annotation.\n",
        "    # Since there isn't a direct 'answer' field, we'll use the caption as a reference\n",
        "    # for the ground truth, which also resolves the TypeError.\n",
        "    ground_truth = sample['caption']\n",
        "    # Takes the first, most common answer\n",
        "    try:\n",
        "        # Run the Hybrid VQA Pipeline\n",
        "        predicted_answer, ocr_output = hybrid_vqa(image, question)\n",
        "        # Print Results\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(f\"TEST {i+1}:\")\n",
        "        print(f\"  Question: {question}\")\n",
        "        print(f\"  Ground Truth: {ground_truth}\")\n",
        "        print(f\"  OCR Output (snippet): {ocr_output[:70]}...\")\n",
        "        print(f\"  PREDICTED: {predicted_answer}\")\n",
        "        print(\"-\"*50)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during VQA for sample {i+1}: {e}\")\n",
        "        continue\n",
        "print(\"\\n\" + \"*\"*60)\n",
        "print(\"TESTING COMPLETE\")\n",
        "print(\"*\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "# Conclusion\n",
        "###This notebook demonstrates how to build a specialized VQA system for label reading by incorporating text extraction (OCR) into the prompt construction. This hybrid approach is key to achieving high accuracy on detail-oriented tasks like reading ingredient lists or nutrition facts.\n",
        "###The automated testing loop allows you to quickly evaluate the model's performance on real-world data from the VizWiz dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}